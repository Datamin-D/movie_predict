{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie_predict_Webcrawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1u-xhfFVw8H3SP0yaKxicNJrBfa_aqQeT",
      "authorship_tag": "ABX9TyMf+BydnY2//olkYqPE6GVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datamin-D/movie_predict/blob/master/Movie_predict_Webcrawling(mojo%3EIMDb%ED%8C%90).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqzhJuc0TT5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import openpyxl\n",
        "import xlwt,xlrd\n",
        "from google.colab import files\n",
        "import time\n",
        "from random import randint\n",
        "import re\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0oIMSEQuFEW",
        "colab_type": "code",
        "outputId": "2c36ae3e-a2a7-4bb7-922e-488db17802c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#어?! 다시왔어?! package\n",
        "#import\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import openpyxl\n",
        "import xlwt,xlrd\n",
        "from google.colab import files\n",
        "import time\n",
        "from random import randint\n",
        "import re\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "#installing selenium, prepare to connect \n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "#get the pickle\n",
        "#Boxoffice mojo에서 수집한 movie_titles 위 코드 돌리지말고 바로 접근.\n",
        "with open(\"/content/drive/My Drive/movie_predict/movie_titles(2).txt\",\"rb\") as f:\n",
        "  movie_titles = pickle.load(f)\n",
        "print(movie_titles[0],len(movie_titles))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [856 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,829 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,387 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [933 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,230 kB]\n",
            "Get:19 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [882 kB]\n",
            "Fetched 7,388 kB in 6s (1,194 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 77.3 MB of archives.\n",
            "After this operation, 264 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 81.0.4044.138-0ubuntu0.18.04.1 [1,095 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 81.0.4044.138-0ubuntu0.18.04.1 [68.9 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 81.0.4044.138-0ubuntu0.18.04.1 [3,231 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 81.0.4044.138-0ubuntu0.18.04.1 [4,079 kB]\n",
            "Fetched 77.3 MB in 10s (8,086 kB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_81.0.4044.138-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: use options instead of chrome_options\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Greetings from Tim Buckley (2013) 12146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeq_KXjy4osF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1. feature값 비교. + 연도별로 파일 저장할 수 있게하고 오류 뜬 연도는 별도 표시.  오전 -피클에서 수집한거 100 프린트하고 하나하나 대조.(겹친거 없나)\n",
        "2. 수집한 데이터 중간 중간에 저장하는 법?\n",
        "3. 전처리 - 어떤 영화를 '성공'으로 정의할지.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD61hig6UIlh",
        "colab_type": "code",
        "outputId": "0e4668a4-8244-447e-8b6f-fab315942edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "#installing selenium, prepare to connect \n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 2s (125 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (81.0.4044.138-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 61 not upgraded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of chrome_options\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Wdh_tbVXlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Boxoffice mojo > extract movie titles by years(skip pickle has the movie titles info) \n",
        "\n",
        "#수집 완료.\n",
        "\n",
        "# 수집할 영화 연도 범위\n",
        "released_years = []\n",
        "released_months = []\n",
        "\n",
        "years = [released_years.append(i) for i in range(1921,2019)] #연\n",
        "months = [released_months.append(i) for i in range(1,13)] #월\n",
        "\n",
        "#for문의 내포. [명령 for i in range(시작,끝,step)]\n",
        "#print(released_years)\n",
        "\n",
        "# raw data 수집,parsing, 영화 title저장\n",
        "movie_list = []\n",
        "for year in released_years:\n",
        "  for month in released_months:\n",
        "    #print(genre)\n",
        "    raw = requests.get(\"https://www.boxofficemojo.com/calendar/\"+str(year)+\"-\"+str(month)+\"-01/\")\n",
        "      #print(raw)해서 response 200이면 정상.\n",
        "      #주소니까 str로 해줘야. \n",
        "\n",
        "      #print(raw.text)\n",
        "      #.text하면 --> URL소스코드 알아내기\n",
        "\n",
        "    html = BeautifulSoup(raw.text,\"html.parser\")\n",
        "      #변수 = BeautifulSoup(소스코드, 'html.parser')-->태그 기준으로 파싱해줌\n",
        "      #print(html)\n",
        "\n",
        "  # 1. 컨테이너 \n",
        "    container = html.select(\"div.a-section.a-spacing-none.mojo-schedule-release-details\")\n",
        "  # 2. Title    \n",
        "    for cont in container:\n",
        "      movie_title = cont.select_one(\"a.a-link-normal h3\").text.strip()\n",
        "      #자식 \">\" 자손 \" \"(띄어쓰기) ex) ul.type01 > li\n",
        "      #.strip-->양쪽 공백 제거\n",
        "\n",
        "      movie_list.append(movie_title+\" \"+\"(\"+str(year)+\")\")\n",
        "      #태그이름.class, 태그이름#id\n",
        "print(len(movie_list))\n",
        "\n",
        "#movie list 고유값만 추리기. set성질 이용.\n",
        "my_set = set(movie_list)\n",
        "movie_titles = list(my_set)\n",
        "print(len(movie_titles))\n",
        "\"\"\"\n",
        "with open (\"movie_titles.txt\",\"wb\") as f:\n",
        "  pickle.dump(movie_titles,f)\n",
        "#movie_titles 표현형식: 영화이름, (연도) \n",
        "movie_titles[0]\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTReKExG2vrd",
        "colab_type": "code",
        "outputId": "35adb52d-03ea-47d2-edfb-1e7577f2a2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#movie_tites 객체 다운로드\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IctjEmqdA3fV",
        "colab_type": "code",
        "outputId": "d22d0085-0701-4ff6-c0d8-0118b5e0d915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Boxoffice mojo에서 수집한 movie_titles 위 코드 돌리지말고 바로 접근.\n",
        "with open(\"/content/drive/My Drive/movie_predict/movie_titles(2).txt\",\"rb\") as f:\n",
        "  movie_titles = pickle.load(f)\n",
        "print(movie_titles[0],len(movie_titles))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Greetings from Tim Buckley (2013) 12146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZJUEivd6anX",
        "colab_type": "code",
        "outputId": "0b9fdc3e-882b-4499-b3de-cc31f5da1d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#영화 이름, 연도 분리..-->영화 이름, 연도와 매칭..굳이?  이미 IMDb 검색 기능에 구현되어 있을 것. --> abort.\n",
        "\"\"\"\n",
        "indv_title = []\n",
        "indv_year =  []\n",
        "for i in range(len(movie_titles)):\n",
        "  p = re.compile(r'([\\'\\\",/.a-zA-z0-9_ :!?\\+\\-\\&\\#\\$\\@\\%\\-\\=\\^\\`\\(\\)]*) (\\([0-9]+\\))') #정규표현식으로 영화 이름 분리\n",
        "  target = movie_list[i]\n",
        "  s = p.search(target) #match는 처음부터 매칭해보고 없으면 진행 안함. search는 문자열 전체를 검색.(앞이 매칭 안되어도.)\n",
        "  indv_title.append(s.group(1)) #i번째영화이름\n",
        "  indv_year.append(s.group(2))  #i번째 영화의 연도(인덱스로 찾을 수 있음.)\n",
        "  m = p.search(target)\n",
        "print(indv_year[0])\n",
        " #이 코드 바탕으로imdb검색한 후 이름,연도 매칭하도록 코딩ㄱ *\n",
        " \"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nindv_title = []\\nindv_year =  []\\nfor i in range(len(movie_titles)):\\n  p = re.compile(r\\'([\\'\",/.a-zA-z0-9_ :!?\\\\+\\\\-\\\\&\\\\#\\\\$\\\\@\\\\%\\\\-\\\\=\\\\^\\\\`\\\\(\\\\)]*) (\\\\([0-9]+\\\\))\\') #정규표현식으로 영화 이름 분리\\n  target = movie_list[i]\\n  s = p.search(target) #match는 처음부터 매칭해보고 없으면 진행 안함. search는 문자열 전체를 검색.(앞이 매칭 안되어도.)\\n  indv_title.append(s.group(1)) #i번째영화이름\\n  indv_year.append(s.group(2))  #i번째 영화의 연도(인덱스로 찾을 수 있음.)\\n  m = p.search(target)\\nprint(indv_year[0])\\n #이 코드 바탕으로imdb검색한 후 이름,연도 매칭하도록 코딩ㄱ *\\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7Z7RRHLTVrb",
        "colab_type": "code",
        "outputId": "cfe47b1f-063c-4e48-bfd8-874fdc10bf75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "# Searching movie webpages in IMDb \n",
        "\n",
        "#Features 목록\n",
        "Columns = [\"Movie_title\",\"Genre\",\"Rate\",\"Rating_count\",\"Run_time\",\"Budget\",\"Gross_USA\",\"Opening Weekend USA\",\"Cumulative Worldwide Gross\",\"Language\",\"Directors\",\"Writers\",\"Stars\",\"Certificate\",\"Released_date\",\"Domestic_distributor\"]\n",
        "\n",
        "#DataFrame생성  12개.\n",
        "df0 = pd.DataFrame(columns=Columns)\n",
        "df1 = pd.DataFrame(columns=Columns)\n",
        "df2 = pd.DataFrame(columns=Columns)\n",
        "df3 = pd.DataFrame(columns=Columns)\n",
        "df4 = pd.DataFrame(columns=Columns)\n",
        "df5 = pd.DataFrame(columns=Columns)\n",
        "df6 = pd.DataFrame(columns=Columns)\n",
        "df7 = pd.DataFrame(columns=Columns)\n",
        "df8 = pd.DataFrame(columns=Columns)\n",
        "df9 = pd.DataFrame(columns=Columns)\n",
        "df10 = pd.DataFrame(columns=Columns)\n",
        "df11 = pd.DataFrame(columns=Columns)\n",
        "\n",
        "#영화 999개씩 나누어 파일로 저장.\n",
        "for ind_flno in range(0,2): #12번에 나눠서 수집. ***이거 바꾸고다시다운\n",
        "  unit = 6 #999로 바꾸기***\n",
        "  first_no = unit * ind_flno\n",
        "  last_no = unit* (ind_flno + 1)\n",
        "  unit_titles = movie_titles[first_no:last_no] #999개영화 뽑아넣은 타이틀\n",
        "\n",
        "  save_file_name = \"movie_data_\"+str(ind_flno)+\".csv\"\n",
        "  print(save_file_name)\n",
        "  df_name = [df0,df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11][ind_flno] #끊길 시 ***이거 바꾸고다시다운\n",
        "  #df_name = [df0,df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11][ind_flno]  #df원래\n",
        "  \n",
        "#영화 페이지 접근\n",
        "  for i in range(len(unit_titles)):#unit수 만큼의 영화만 접근 \n",
        "  #for i in range(6): #실험용\n",
        "    url = \"https://www.imdb.com/find?q=\"+str(unit_titles[i])+\"&ref_=nv_sr_sm\" #movie_title에 영화 이름 1개 들어가야함. \n",
        "    #print(\"real영화 제목:\",movie_titles[i])\n",
        "    wd.get(url)\n",
        "    time.sleep(randint(1,3))\n",
        "    #print(wd.page_source)  # results\n",
        "    try:\n",
        "      click_movie = wd.find_elements_by_css_selector('td.result_text > a') #영화 검색창 후 이름 클릭\n",
        "      click_movie[0].click() #클릭이 안되는 경우, no result found. pass하게 코딩.\n",
        "    \n",
        "      time.sleep(randint(1,3))\n",
        "    except IndexError: #영화 정보가 없어서 클릭이 안되면.\n",
        "      continue \n",
        "    # 여기부터 영화 개별 웹페이지 진입 성공, 자료수집  \n",
        "  #제외할 영화\n",
        "    try:\n",
        "      targets = wd.find_elements_by_css_selector(\"#title-overview-widget > div.vital > div.title_block > div > div.titleBar > div.title_wrapper  > div > a\")\n",
        "      abandoned_genre = [\"Short\",\"Video game\",\"Episode\",\"TV Movie\",\"TV Special\",\"TV Mini-Series\",\"TV Movie\",\"TV Series\",\"Video\",\"TV Series\"] #제외할 데이터타입\n",
        "      \n",
        "\n",
        "      def ab_collector():\n",
        "        for target in targets:\n",
        "          time.sleep(1)\n",
        "          for ab in abandoned_genre:\n",
        "            time.sleep(1)\n",
        "            if ab.lower() in target.text.lower(): #lower() 괄호 꼭 붙여줘야함.\n",
        "              return True #함수는 return을 만나는 순간 결과값 돌려주고 함수 빠져나감... 코딩 주의\n",
        "\n",
        "      if ab_collector():\n",
        "        continue\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "    except:\n",
        "      pass  \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    #Movie_title\n",
        "    Movie_title = []\n",
        "    try:\n",
        "      imdb_title = wd.find_element_by_css_selector(\"#title-overview-widget > div.vital > div.title_block > div > div.titleBar > div.title_wrapper > h1\").text\n",
        "      if imdb_title == unit_titles[i]:\n",
        "        Movie_title = imdb_title\n",
        "      else:\n",
        "        continue #이렇게 루프 뛰어넘어도 df index에는 공백으로 흔적 남아있음\n",
        "      #print(wd.current_url)\n",
        "    except NoSuchElementException:\n",
        "      Movie_title.append(unit_titles[i])    \n",
        "    #print(Movie_title)\n",
        "      \n",
        "\n",
        "  #Genre - 수집 중 날짜까지 리스트 마지막에 수집되어서 index[-1]지우는 식으로 코딩함. 성공!\n",
        "    Genre = []\n",
        "    try:\n",
        "      genre = wd.find_elements_by_css_selector(\"div.subtext a\")\n",
        "      for gen in range(len(genre)-1):\n",
        "        Genre.append(genre[gen].text)\n",
        "    except:\n",
        "      Genre = \"NA\"\n",
        "\n",
        "  #Rate(전체 별점) \n",
        "    Rate = []\n",
        "    try:\n",
        "      Rate.append(wd.find_element_by_css_selector(\"div.ratingValue span:nth-of-type(1):first-child\").text) # nth-of-type(n번째) n은 1부터 시작, first-child하면 부모 모든 요소 중 첫번째 자식만 선택 \n",
        "    except NoSuchElementException:\n",
        "      Rate.append(\"NA\")\n",
        "  #Rating_Count\n",
        "    Rating_count = \"NA\"\n",
        "    try:\n",
        "      Rating_count = wd.find_element_by_css_selector(\"#title-overview-widget > div.vital > div.title_block > div > div.ratings_wrapper > div.imdbRating > a > span\").text\n",
        "    except:\n",
        "      Rating_count = \"NA\"\n",
        "\n",
        "  #Run_time\n",
        "    Run_time = []\n",
        "    try:\n",
        "      Run_time.append(wd.find_element_by_css_selector('#title-overview-widget > div.vital > div.title_block > div > div.titleBar > div.title_wrapper > div > time').text)\n",
        "    except NoSuchElementException:\n",
        "      continue\n",
        "\n",
        "    \n",
        "\n",
        "  #선택자#titleDetails >.txt-block\n",
        "    Bcontainer = wd.find_elements_by_css_selector(\"#titleDetails >.txt-block\")\n",
        "#Budget \n",
        "    Budget = \"NA\"\n",
        "    try:\n",
        "      for ind_Budget in range(len(Bcontainer)):\n",
        "        if \"Budget:\" in Bcontainer[ind_Budget].text: #if문에 해당하는 내용이 없다해서 오류가 일어나지는 않음.\n",
        "          budget = Bcontainer[ind_Budget].text\n",
        "          Budget = budget.split(\":\")[1]\n",
        "          \n",
        "          break  #break는 조건이 참이면 반복문 종료. cf)continue는 아래의 코드를 실행않고 다음 반복으로 넘어감.\n",
        "        else:\n",
        "          Budget = \"NA\"\n",
        "    except:\n",
        "      Budget = \"NA\"\n",
        "#Gross_USA \n",
        "    Gross_USA = \"NA\"\n",
        "    try:\n",
        "      for ind_gusa in range(len(Bcontainer)):\n",
        "        if \"Gross USA:\" in Bcontainer[ind_gusa].text:\n",
        "          gusa = Bcontainer[ind_gusa].text\n",
        "          Gross_USA = gusa.split(\":\")[1]\n",
        "          break\n",
        "        else:\n",
        "          Gross_USA = \"NA\"\n",
        "    except:\n",
        "      Gross_USA = \"NA\"\n",
        "#Opening Weekend USA \n",
        "    Opening_Weekend_USA  = \"NA\"\n",
        "    try:\n",
        "      for ind_OWU in range(len(Bcontainer)):\n",
        "        if \"Opening Weekend USA:\" in Bcontainer[ind_OWU].text:\n",
        "          OWU = Bcontainer[ind_OWU].text\n",
        "          Opening_Weekend_USA = OWU.split(\":\")[1]\n",
        "          break\n",
        "        else:\n",
        "          Opening_Weekend_USA = \"NA\"\n",
        "    except:\n",
        "      Opening_Weekend_USA = \"NA\"\n",
        "\n",
        "#Cumulative Worldwide Gross \n",
        "    Cumulative_Worldwide_Gross  = \"NA\"\n",
        "    try:\n",
        "      for ind_CWG in range(len(Bcontainer)):\n",
        "        if \"Cumulative Worldwide Gross:\" in Bcontainer[ind_CWG].text:\n",
        "          CWG = Bcontainer[ind_CWG].text\n",
        "          Cumulative_Worldwide_Gross = CWG.split(\":\")[1]\n",
        "          break\n",
        "        else:\n",
        "          Cumulative_Worldwide_Gross = \"NA\"\n",
        "    except:\n",
        "      Cumulative_Worldwide_Gross = \"NA\"\n",
        "    \n",
        "  #Language: 로 시작하는 컨테이너찾기, 적당히 인덱싱해서 Language찾기. \n",
        "    Language = []\n",
        "    try:\n",
        "      find_language = wd.find_elements_by_css_selector(\"#titleDetails > div.txt-block\")\n",
        "      for ind_lang in range(len(find_language)):\n",
        "        if find_language[ind_lang].text.strip().startswith(\"Language:\"):\n",
        "          Lang = find_language[ind_lang].text.strip()\n",
        "      Language =  Lang[9:].strip().split(\"|\")\n",
        "      \n",
        "    except NoSuchElementException:\n",
        "      Language.append(\"NA\")\n",
        "\n",
        "\n",
        "    wd.close #개별 영화 페이지 닫기\n",
        "    \n",
        "  #See more credits접근 try except처리.***여기 급히 수정함.\n",
        "    try:\n",
        "      indv_movie_url = wd.current_url # 개별 영화 웹페이지 url\n",
        "      target = str(indv_movie_url)\n",
        "      p = re.compile(r\"(https://www.imdb.com/title/)([a-zA-z]{2}[0-9]{7})(/\\?ref_=fn_al_tt_1)\")\n",
        "      m = p.match(target)\n",
        "      movie_code = m.group(2) #영화코드\n",
        "      url = \"https://www.imdb.com/title/\"+movie_code+\"/fullcredits/?ref_=tt_ov_st_sm\"\n",
        "      wd.get(url) #See full cast & crew 페이지 열기\n",
        "      time.sleep(randint(1,2))\n",
        "      cu = wd.current_url\n",
        "    except:\n",
        "      continue\n",
        "      \n",
        "\n",
        "  #Directors\n",
        "    Directors_ch = []\n",
        "    Directors = []\n",
        "    try:\n",
        "      director = wd.find_elements_by_css_selector(\"#fullcredits_content > table:nth-child(2) > tbody>tr\")\n",
        "      for ind_directors in range(len(director)):\n",
        "        Directors_ch.append(director[ind_directors].text)\n",
        "      for ind_Directors in range(len(Directors_ch)):\n",
        "        target = Directors_ch[ind_Directors]\n",
        "        p = re.compile(\"[\\-. a-zA-z0-9]+\")\n",
        "        m = p.match(target)\n",
        "        \n",
        "        Director = m.group().strip()\n",
        "        Directors.append(Director) #director_list\n",
        "      #print(Directors) \n",
        "    except AttributeError:\n",
        "      Directors.append(\"NA\")\n",
        "\n",
        "  # Writers\n",
        "    Writers = []\n",
        "    try:\n",
        "      Writing_credits = wd.find_elements_by_css_selector(\"#fullcredits_content > table:nth-child(4) > tbody > tr > td.name > a\") #Writing Credits 박스 접근. 아래 자식으로 작가 이름 크롤링ㄱ \n",
        "      for ind_writer in range(len(Writing_credits)):\n",
        "        Writers.append(Writing_credits[ind_writer].text)\n",
        "    except:\n",
        "      Writers = \"NA\"\n",
        "\n",
        "    #if #fullcredits_content h4.dataHeaderWithBorder == Writing Credits 거기 멈춰서 안에 있는 박스 > ..는 복잡.\n",
        "    #걍 nth-ofchild 기능으로 선택자 처리함.\n",
        "\n",
        "    # Stars 상위16명까지. -->ㅡmin (2003)시도 완료\n",
        "    #홀수 번호 배우들 수집 \n",
        "    Stars_ch = []\n",
        "    Stars_odd = []\n",
        "    Stars_even = []\n",
        "    Stars = []\n",
        "\n",
        "    try:\n",
        "      container_odd = wd.find_elements_by_css_selector(\"#fullcredits_content > table.cast_list > tbody > tr.odd\") #홀수번호 배우들 데이터 수집\n",
        "      for ind_oddstar in range(9): #8번째 홀수까지 반복, 15\n",
        "        Stars_odd.append(container_odd[ind_oddstar].text)\n",
        "    except IndexError: \n",
        "      pass\n",
        "\n",
        "    #짝수 번호 배우들 수집\n",
        "    try:\n",
        "      container_even = wd.find_elements_by_css_selector(\"#fullcredits_content > table.cast_list > tbody > tr.even\") #짝수번호 배우들 데이터 수집\n",
        "      for ind_evenstar in range(9): #8번째 짝수까지 반복,16 \n",
        "        Stars_even.append(container_even[ind_evenstar].text)\n",
        "    except IndexError: \n",
        "      pass\n",
        "    #홀,짝 합치기\n",
        "    try:\n",
        "\n",
        "      for ind_star in range(8):\n",
        "        Stars_ch.append(Stars_odd[ind_star])\n",
        "        Stars_ch.append(Stars_even[ind_star])\n",
        "      #print(Stars_ch) 합치기 완료\n",
        "    except IndexError: #배우 15명 미만일 시 pass.\n",
        "      pass\n",
        "\n",
        "    \n",
        "    #배우\"...캐릭터\"지우기는 정규 표현식으로 처리.\n",
        "    try:\n",
        "      for ind_ch in range(len(Stars_ch)):\n",
        "        target = Stars_ch[ind_ch]\n",
        "        p = re.compile(r\"([\\-\\.\\' a-zA-z0-9]+)( ... )([\\-\\.\\' a-zA-z0-9]+)\")\n",
        "        m = p.match(target)\n",
        "        Star = m.group(1).strip()\n",
        "        Stars.append(Star) #배우목록\n",
        "        #print(Star)\n",
        "    except AttributeError:\n",
        "      Star\n",
        "    wd.close ##See full cast & crew 페이지 닫기\n",
        "\n",
        "  #Certificate 급히 수정*** try except,continue\n",
        "    Certificate = []\n",
        "    #indv_movie_url는  개별 영화 웹페이지 url\n",
        "    try:\n",
        "      target = str(indv_movie_url)\n",
        "      p = re.compile(r\"(https://www.imdb.com/title/)([a-zA-z]{2}[0-9]{7})(/\\?ref_=fn_al_tt_1)\")\n",
        "      m = p.match(target)\n",
        "      movie_code = m.group(2) #영화코드\n",
        "      #Certificate > clicking see more button using Selenium\n",
        "      url = \"https://www.imdb.com/title/\"+movie_code+\"/parentalguide?ref_=tt_stry_pg#certification\" #개별영화 페이지 movie_title에 영화 이름 1개 들어가야함. \n",
        "      wd.get(url) #certificate 페이지 열기\n",
        "      time.sleep(randint(1,3))\n",
        "      #컨테이너 li.ipl-inline-list__item a\n",
        "      countries = wd.find_elements_by_css_selector(\"tr#certifications-list > td > ul.ipl-inline-list >li.ipl-inline-list__item > a\")\n",
        "      try:\n",
        "        for c_ind in range(len(countries)):\n",
        "          if \"United States\" in countries[c_ind].text:\n",
        "            US_cert = countries[c_ind].text\n",
        "            nation_cert = US_cert.split(\":\")\n",
        "            Certificate.append(nation_cert[1])\n",
        "          else:\n",
        "            pass #다른 나라들은 pass. \n",
        "      except:\n",
        "        Certificate.append(\"NA\")\n",
        "      #print(Certificate)\n",
        "    except:\n",
        "      Certificate.append(\"NA\")\n",
        "    wd.close #certificate 페이지 닫기\n",
        "    #Released_date imdb에서 가져오기.\n",
        "\n",
        "\n",
        "    furl = \"https://www.imdb.com/title/\"+movie_code+\"/releaseinfo?ref_=tt_ov_inf\"\n",
        "    wd.get(furl)#release info 열기\n",
        "    time.sleep(randint(1,2))\n",
        "    #개봉한 나라 갯수 (=행 갯수)\n",
        "    release_rows = wd.find_elements_by_css_selector(\"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr > td.release-date-item__country-name > a\") #개봉한 나라 이름 선택\n",
        "    #print(len(release_rows)) 개봉한 나라 갯수 - 성공\n",
        "    def cond_empty(row):\n",
        "      try:\n",
        "        wd.find_element_by_css_selector(\"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr:nth-child(\"+ str(row + 1) +\") > td.release-date-item__attributes--empty\").text\n",
        "        return True\n",
        "      except:\n",
        "        return False #성공 row12로 놓고해봄.\n",
        "    def cond_limited(row):\n",
        "      try:\n",
        "        limited_selector = \"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr:nth-child(\" + str(row + 1) + \") > td.release-date-item__attributes\"\n",
        "        if wd.find_element_by_css_selector(limited_selector).text.startswith(\"(limited)\"):#상영이 limited\n",
        "           return True\n",
        "      except:\n",
        "        return False\n",
        "\n",
        "\n",
        "    # 개봉일자가 있는경우(전국,limited)\n",
        "    try:\n",
        "      for row in range(len(release_rows)-1): #각 행마다 검사 + 나라이름있는 행의 row 기억,해당 행이 USA로 시작,3번째 값이 비면 2번째 값 찾기 \n",
        "        nation_selector= \"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr:nth-child(\"+ str(row+1) +\") > td.release-date-item__country-name\" #국가 이름 선택자\n",
        "        cond_USA = wd.find_element_by_css_selector(nation_selector).text.startswith(\"USA\") #개봉국가가 USA\n",
        "        released_date_selector = \"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr:nth-child(\"+ str(row+1) + \") > td.release-date-item__date\" #중간 칸에 있는 released date\n",
        "        if cond_USA and cond_empty(row): #US & 전국개봉\n",
        "          Released_date = wd.find_element_by_css_selector(released_date_selector).text\n",
        "          print(Movie_title,Released_date)\n",
        "        else: #US& limited개봉\n",
        "          if cond_USA and cond_limited(row):\n",
        "           Released_date = wd.find_element_by_css_selector(released_date_selector).text\n",
        "        #American Animals (2018)의 경우 limited, 전국 개봉 둘다 있는데 뭐로 수집했는지 확인하기. + 왜 NA로 떴나 문제해결.\n",
        "\n",
        "    #개봉일자가 없는 경우 /오류 일어난 경우\n",
        "    except NoSuchElementException:\n",
        "      Released_date = \"NA\"\n",
        "\n",
        "    wd.close #mojo-release info등 닫기\n",
        "    time.sleep(randint(1,2))\n",
        "\n",
        "    #Domestic Distributor 도 수집.\n",
        "    durl = \"https://www.boxofficemojo.com/title/\" + movie_code+ \"/?ref_=bo_se_r_1\"\n",
        "    wd.get(durl)\n",
        "    Domestic_distributor = \"NA\"\n",
        "    find_distributor = wd.find_elements_by_css_selector(\"#a-page > main > div > div.a-section.a-spacing-none.mojo-gutter.mojo-summary-table > div.a-section.a-spacing-none.mojo-summary-values.mojo-hidden-from-mobile > div.a-section.a-spacing-none\")\n",
        "    try:\n",
        "      for ind_distributor in  find_distributor: #domestic distributor행 찾기\n",
        "        if ind_distributor.text.startswith(\"Domestic Distributor\"):\n",
        "          distributor_line = ind_distributor.text\n",
        "          target = distributor_line\n",
        "          p = re.compile(r'(Domestic Distributor)\\s([\\'\\\",/.a-zA-z0-9_ :!?\\+\\-\\&\\#\\$\\@\\%\\-\\=\\^\\`\\(\\)]*)') #배급사 이름 추리기\n",
        "          m = p.match(target) #match는 처음부터 매칭해보고 없으면 진행 안함. search는 문자열 전체를 검색.(앞이 매칭 안되어도.)\n",
        "          Domestic_distributor = m.group(2)\n",
        "        else:\n",
        "          pass\n",
        "    except:\n",
        "      Domestic_distributor = \"NA\"\n",
        "    df_name.loc[i] = Movie_title,Genre,Rate,Rating_count,Run_time,Budget,Gross_USA,Opening_Weekend_USA,Cumulative_Worldwide_Gross,Language,Directors,Writers,Stars,Certificate, Released_date,Domestic_distributor #i번째 영화, 그에 해당하는 데이터\n",
        "    #loc은 행 데이터 채우기.\n",
        "    #print(Movie_title,Genre,Rate,Run_time,Budget,Gross_USA,Cumulative_Worldwide_Gross,Opening_Weekend_USA,Language,Directors,Writers,Stars,Certificate)\n",
        "    wd.close #domestic distributor info 닫기\n",
        "    time.sleep(randint(1,2))\n",
        "  df_name.to_csv(save_file_name) #unit_titles만큼 모아서 하나로 저장\n",
        "  time.sleep(randint(2,4))\n",
        "  files.download(save_file_name)\n",
        "  \n",
        "  #print(df)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movie_data_0.csv\n",
            "American Animals (2018) 14 August 2018\n",
            "Step Up Revolution (2012) 27 July 2012\n",
            "Bad News Bears (2005) 22 July 2005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-832c76639e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0mdf_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#unit_titles만큼 모아서 하나로 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m   \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[0;31m#print(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdXoW3Cr6jUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coll = [\"no\",\"tw\"]\n",
        "file_na = 'fn.csv'\n",
        "mimi = pd.DataFrame(columns = coll)\n",
        "mimi.to_csv(file_na)\n",
        "#files.download(save_file_name)\n",
        "files.download(file_na)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdiVaiPeUY8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" (무시하셔도 됩니다!)\n",
        "# 엑셀파일 만들기\n",
        "file_name = 'workbook.xls'\n",
        "wb = xlwt.Workbook()\n",
        "ws_1 = wb.add_sheet('sheet0',cell_overwrite_ok = True)\n",
        "ws_1 = wb.get_sheet(0)\n",
        "ws_1.write(1,1,'영화제목')\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su9h8KFVROiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Writers,Stars 개별웹페이지에서 바로 가져오는법(See full cast 말고.) (무시하셔도 됩니다!)\n",
        "\"\"\"\n",
        "# Writers\n",
        "Writer = []\n",
        "writer = Container[1].select(\"a\")\n",
        "for i in range(len(writer)):\n",
        "  Writer.append(writer[i].text.strip())\n",
        "\n",
        "  # n more credits 지우기\n",
        "for i in range(len(Writer)):\n",
        "  if \"more credits\" or \"more credit\" in Writer[i]:\n",
        "   Wdelete_i = i # Writer에서 지워야하는 인덱스\n",
        "  else: \n",
        "    pass\n",
        "del Writer[Wdelete_i] \n",
        "\n",
        "  # Top2 in Writers \n",
        "if len(Writer) > 2: #3명이상이면\n",
        "  Writer = Writer[0:2]\n",
        "print(Writer)\n",
        "\n",
        "# Stars\n",
        "Stars = []\n",
        "stars = Container[2].select(\"a\")\n",
        "for i in range(len(stars)):\n",
        "  Stars.append(stars[i].text.strip())\n",
        "\n",
        "  # See full cast & crew 지우기\n",
        "for i in range(len(Stars)):\n",
        "  if \"See full cast & crew\" in Stars[i]:\n",
        "   Sdelete_i = i # Stars에서 지워야하는 인덱스\n",
        "  else: \n",
        "    pass\n",
        "del Stars[Sdelete_i] \n",
        "  #Top 2 stars\n",
        "if len(Stars) > 2: #3명이상이면\n",
        "  Stars = Stars[0:2] # 2명만 extract\n",
        "print(Stars)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5iRR0eeUhKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"(무시하셔도 됩니다!)\n",
        "#엑셀파일 다운받기\n",
        "for i in range(len(new_movie_titles)):\n",
        "  ws_1.write(i+1,1,new_movie_titles[i])\n",
        "      \n",
        "      \n",
        "wb.save(file_name)\n",
        "files.download(file_name)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp0Z39uKgMMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "알고리즘 비교\n",
        "목표: USA고, 아무것도 세번째 칸에 없어야함.\n",
        "1. 첫째 선을 비교, USA 를 찾는다\n",
        "2. 해당 row에 세번째칸에 아무것도 없으면 두번째 값을 저장한다. (개봉일)\n",
        "\n",
        "1.전체 선을 수집, USA로 시작하는 것만 찾기\n",
        "2. USA 중 \n",
        "nth:child 진화 과제로 채택.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXSf4XthFrGa",
        "colab_type": "code",
        "outputId": "aaea8138-d42b-412f-829b-26fbaab9a639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Released_date찾기! IMDB\n",
        "# Searching movie webpages in IMDb \n",
        "\n",
        "#Features 목록\n",
        "Columns = [\"Movie_title\",\"Released_date\"]\n",
        "\n",
        "#DataFrame생성  12개.\n",
        "df0 = pd.DataFrame(columns=Columns)\n",
        "df1 = pd.DataFrame(columns=Columns)\n",
        "df2 = pd.DataFrame(columns=Columns)\n",
        "df3 = pd.DataFrame(columns=Columns)\n",
        "df4 = pd.DataFrame(columns=Columns)\n",
        "df5 = pd.DataFrame(columns=Columns)\n",
        "df6 = pd.DataFrame(columns=Columns)\n",
        "df7 = pd.DataFrame(columns=Columns)\n",
        "df8 = pd.DataFrame(columns=Columns)\n",
        "df9 = pd.DataFrame(columns=Columns)\n",
        "df10 = pd.DataFrame(columns=Columns)\n",
        "df11 = pd.DataFrame(columns=Columns)\n",
        "\n",
        "#영화 999개씩 나누어 파일로 저장.\n",
        "for ind_flno in range(0,2): #12번에 나눠서 수집. ***이거 바꾸고다시다운\n",
        "  unit = 2 #999로 바꾸기***\n",
        "  first_no = unit * ind_flno\n",
        "  last_no = unit* (ind_flno + 1)\n",
        "  unit_titles = movie_titles[first_no:last_no] #999개영화 뽑아넣은 타이틀\n",
        "\n",
        "  save_file_name = \"movie_data_\"+str(ind_flno)+\".csv\"\n",
        "  print(save_file_name)\n",
        "  df_name = [df0,df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11][ind_flno] #끊길 시 ***이거 바꾸고다시다운\n",
        "  #df_name = [df0,df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11][ind_flno]  #df원래\n",
        "  \n",
        "#영화 페이지 접근\n",
        "  for i in range(len(unit_titles)):#unit수 만큼의 영화만 접근 \n",
        "  #for i in range(6): #실험용\n",
        "    url = \"https://www.imdb.com/find?q=\"+str(unit_titles[i])+\"&ref_=nv_sr_sm\" #movie_title에 영화 이름 1개 들어가야함. \n",
        "    #print(\"real영화 제목:\",movie_titles[i])\n",
        "    wd.get(url) #영화 페이지 열기\n",
        "    time.sleep(randint(1,3))\n",
        "    #print(wd.page_source)  # results\n",
        "    try:\n",
        "      click_movie = wd.find_elements_by_css_selector('td.result_text > a') #영화 검색창 후 이름 클릭\n",
        "      click_movie[0].click() #클릭이 안되는 경우, no result found. pass하게 코딩.\n",
        "    \n",
        "      time.sleep(randint(1,3))\n",
        "    except IndexError: #영화 정보가 없어서 클릭이 안되면.\n",
        "      continue \n",
        "    # 여기부터 영화 개별 웹페이지 진입 성공, 자료수집  \n",
        "\n",
        "\n",
        "    #Movie_title\n",
        "    Movie_title = []\n",
        "    try:\n",
        "      imdb_title = wd.find_element_by_css_selector(\"#title-overview-widget > div.vital > div.title_block > div > div.titleBar > div.title_wrapper > h1\").text\n",
        "      if imdb_title == unit_titles[i]:\n",
        "        Movie_title = imdb_title\n",
        "      else:\n",
        "        continue #이렇게 루프 뛰어넘어도 df index에는 공백으로 흔적 남아있음\n",
        "      #print(wd.current_url)\n",
        "    except NoSuchElementException:\n",
        "      Movie_title.append(unit_titles[i])    \n",
        "    #print(Movie_title)\n",
        "    \n",
        "  #See more credits접근 try except처리.***여기 급히 수정함.\n",
        "    try:\n",
        "      indv_movie_url = wd.current_url # 개별 영화 웹페이지 url\n",
        "      target = str(indv_movie_url)\n",
        "      p = re.compile(r\"(https://www.imdb.com/title/)([a-zA-z]{2}[0-9]{7})(/\\?ref_=fn_al_tt_1)\")\n",
        "      m = p.match(target)\n",
        "      movie_code = m.group(2) #영화코드\n",
        "\n",
        "    except:\n",
        "      continue\n",
        "    wd.close #영화페이지닫기. 여기서는.\n",
        "#여기부터 가져와---\n",
        "    #Released_date imdb에서 가져오기.\n",
        "\n",
        "    furl = \"https://www.imdb.com/title/\"+movie_code+\"/releaseinfo?ref_=tt_ov_inf\"\n",
        "    wd.get(furl)#release info 열기\n",
        "    #개봉한 나라 갯수 (=행 갯수)\n",
        "    release_rows = wd.find_elements_by_css_selector(\"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr > td.release-date-item__country-name > a\") #개봉한 나라 이름 선택\n",
        "    #print(len(release_rows)) 개봉한 나라 갯수 - 성공\n",
        "    def cond_empty(row):\n",
        "      try:\n",
        "        wd.find_element_by_css_selector(\"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr:nth-child(\"+ str(row + 1) +\") > td.release-date-item__attributes--empty\").text\n",
        "        return True\n",
        "      except:\n",
        "        return False #성공 row12로 놓고해봄.\n",
        "    def cond_limited(row):\n",
        "      try:\n",
        "        limited_selector = \"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr:nth-child(\" + str(row + 1) + \") > td.release-date-item__attributes\"\n",
        "        if wd.find_element_by_css_selector(limited_selector).text.startswith(\"(limited)\"):#상영이 limited\n",
        "           return True\n",
        "      except:\n",
        "        return False\n",
        "\n",
        "\n",
        "    # 개봉일자가 있는경우(전국,limited)\n",
        "    try:\n",
        "      for row in range(len(release_rows)-1): #각 행마다 검사 + 나라이름있는 행의 row 기억,해당 행이 USA로 시작,3번째 값이 비면 2번째 값 찾기 \n",
        "        nation_selector= \"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr:nth-child(\"+ str(row+1) +\") > td.release-date-item__country-name\" #국가 이름 선택자\n",
        "        cond_USA = wd.find_element_by_css_selector(nation_selector).text.startswith(\"USA\") #개봉국가가 USA\n",
        "        released_date_selector = \"#releaseinfo_content > table.ipl-zebra-list.ipl-zebra-list--fixed-first.release-dates-table-test-only > tbody > tr:nth-child(\"+ str(row+1) + \") > td.release-date-item__date\" #중간 칸에 있는 released date\n",
        "        if cond_USA and cond_empty(row): #US & 전국개봉\n",
        "          Released_date = wd.find_element_by_css_selector(released_date_selector).text\n",
        "          print(Movie_title,Released_date)\n",
        "        else: #US& limited개봉\n",
        "          if cond_USA and cond_limited(row):\n",
        "           Released_date = wd.find_element_by_css_selector(released_date_selector).text\n",
        "        #American Animals (2018)의 경우 limited, 전국 개봉 둘다 있는데 뭐로 수집했는지 확인하기. + 왜 NA로 떴나 문제해결.\n",
        "\n",
        "    #개봉일자가 없는 경우 /오류 일어난 경우\n",
        "    except NoSuchElementException:\n",
        "      Released_date = \"NA\"\n",
        "    wd.close\n",
        "    df_name.loc[i] = Movie_title,Released_date #i번째 영화, 그에 해당하는 데이터\n",
        "    #loc은 행 데이터 채우기.\n",
        "    #print(Movie_title,Genre,Rate,Run_time,Budget,Gross_USA,Cumulative_Worldwide_Gross,Opening_Weekend_USA,Language,Directors,Writers,Stars,Certificate)\n",
        "    wd.close #mojo-release info등 닫기\n",
        "\n",
        "    df_name.loc[i] = Movie_title,Released_date #i번째 영화, 그에 해당하는 데이터\n",
        "    #loc은 행 데이터 채우기.\n",
        "    #print(Movie_title,Genre,Rate,Run_time,Budget,Gross_USA,Cumulative_Worldwide_Gross,Opening_Weekend_USA,Language,Directors,Writers,Stars,Certificate)\n",
        "    wd.close #mojo-release info등 닫기\n",
        "    time.sleep(randint(1,2))\n",
        "  df_name.to_csv(save_file_name) #unit_titles만큼 모아서 하나로 저장\n",
        "  time.sleep(randint(2,4))\n",
        "  files.download(save_file_name)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movie_data_0.csv\n",
            "American Animals (2018) 14 August 2018\n",
            "movie_data_1.csv\n",
            "Step Up Revolution (2012) 27 July 2012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7esS57BZhhnr",
        "colab_type": "code",
        "outputId": "abd62abc-b2ab-4a55-c3ef-1e504a078c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "    df_name.loc[i] = Movie_title,Released_date #i번째 영화, 그에 해당하는 데이터\n",
        "    #loc은 행 데이터 채우기.\n",
        "    #print(Movie_title,Genre,Rate,Run_time,Budget,Gross_USA,Cumulative_Worldwide_Gross,Opening_Weekend_USA,Language,Directors,Writers,Stars,Certificate)\n",
        "    wd.close #mojo-release info등 닫기\n",
        "\n",
        "    df_name.loc[i] = Movie_title,Released_date #i번째 영화, 그에 해당하는 데이터\n",
        "    #loc은 행 데이터 채우기.\n",
        "    #print(Movie_title,Genre,Rate,Run_time,Budget,Gross_USA,Cumulative_Worldwide_Gross,Opening_Weekend_USA,Language,Directors,Writers,Stars,Certificate)\n",
        "    wd.close #mojo-release info등 닫기\n",
        "    time.sleep(randint(1,2))\n",
        "  df_name.to_csv(save_file_name) #unit_titles만큼 모아서 하나로 저장\n",
        "  time.sleep(randint(2,4))\n",
        "  files.download(save_file_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTXba1vggEZU",
        "colab_type": "code",
        "outputId": "6a422526-3b34-4f99-a605-5c22b355c25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def test(mini):\n",
        "  return(mini+1)\n",
        "\n",
        "mini = 1\n",
        "print(test(mini))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taf1-uVWDtkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}